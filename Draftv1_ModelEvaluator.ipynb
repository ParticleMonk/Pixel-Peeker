{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29737,"status":"ok","timestamp":1699834719446,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"5LWMCFN2XBPQ","outputId":"e19966ce-7c7f-47ef-c372-b433be4b4a01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: google-auth==2.17.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.17.3)\n","Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n","Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n","Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n","Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.3)\n","Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n","Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.31.0)\n","Requirement already satisfied: tornado==6.3.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.3->google-colab) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.3->google-colab) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.3->google-colab) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.3->google-colab) (4.9)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (67.7.2)\n","Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.8.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.2)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.5.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.9.2)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.5.8)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.17.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (1.23.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker==1.5.2->google-colab) (5.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2023.7.22)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (3.11.0)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (2.1.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.9.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (23.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.2.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.18.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.19.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.17.3->google-colab) (0.5.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.12.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.6.4)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.21)\n","Installing collected packages: jedi\n","Successfully installed jedi-0.19.1\n","Mounted at /content/drive\n"]}],"source":["!pip install --upgrade google-colab\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4773,"status":"ok","timestamp":1699834724214,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"LZxELgyhX59t","outputId":"d8c186eb-7b35-4713-e266-7ee3bd997f99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'yolov7'...\n","remote: Enumerating objects: 1197, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 1197 (delta 2), reused 3 (delta 1), pack-reused 1191\u001b[K\n","Receiving objects: 100% (1197/1197), 74.24 MiB | 24.36 MiB/s, done.\n","Resolving deltas: 100% (518/518), done.\n"]}],"source":["!git clone https://github.com/WongKinYiu/yolov7.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5065,"status":"ok","timestamp":1699834729277,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"ydQbXdqKYBCF","outputId":"b5c78900-83f1-497e-c2f0-cc1c2bfc7fa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/yolov7\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n","Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.8.0.76)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (9.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.11.3)\n","Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.1.0+cu118)\n","Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.16.0+cu118)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.66.1)\n","Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.20.3)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.14.1)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (0.12.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n","Collecting thop (from -r requirements.txt (line 36))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2.1.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.59.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.5.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2023.3.post1)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.8.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.9)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.2)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n"]}],"source":["%cd yolov7\n","\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19897,"status":"ok","timestamp":1699834749172,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"Ko8v4NF_kW5m","outputId":"eb0248e0-2be6-41c2-e3f7-fd48c7fe7edc"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 253M/253M [00:13<00:00, 18.3MiB/s]\n"]},{"name":"stdout","output_type":"stream","text":["{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n"]}],"source":["#Grab labels for COCO dataset\n","\n","import requests\n","import os\n","import zipfile\n","import json\n","from tqdm import tqdm\n","\n","# URL for the COCO dataset annotations\n","annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n","\n","# Function to download a file given a URL\n","def download_file(url, filename):\n","    response = requests.get(url, stream=True)\n","    total_size_in_bytes = int(response.headers.get('content-length', 0))\n","    block_size = 1024  # 1 Kibibyte\n","    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n","    with open(filename, 'wb') as file:\n","        for data in response.iter_content(block_size):\n","            progress_bar.update(len(data))\n","            file.write(data)\n","    progress_bar.close()\n","    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n","        print(\"ERROR, something went wrong\")\n","\n","# Download annotations\n","annotations_filename = \"coco_annotations.zip\"\n","download_file(annotations_url, annotations_filename)\n","\n","# Extract annotations\n","with zipfile.ZipFile(annotations_filename, 'r') as zip_ref:\n","    zip_ref.extractall(\"coco_annotations\")\n","\n","# Load the class labels\n","labels_filename = os.path.join(\"coco_annotations\", \"annotations\", \"instances_val2017.json\")\n","with open(labels_filename, 'r') as f:\n","    data = json.load(f)\n","    categories = data['categories']\n","\n","# Extract category names\n","class_labels = {category['id']: category['name'] for category in categories}\n","print(class_labels)\n","\n","# Optionally, save the class labels to a text file\n","with open('coco_class_labels.txt', 'w') as f:\n","    for category_id, category_name in class_labels.items():\n","        f.write(f\"{category_id}: {category_name}\\n\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":253,"status":"ok","timestamp":1699834927319,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"9seJskHNwUtq"},"outputs":[],"source":["#Define useful Functions\n","import cv2\n","import torch\n","import torch.nn.functional as F\n","\n","def preprocess_frame(frame, height, width):\n","    \"\"\"\n","    Preprocess the frame for model input.\n","\n","    Parameters:\n","    frame (ndarray): The frame to be processed.\n","    height (int): The height to resize the frame to.\n","    width (int): The width to resize the frame to.\n","\n","    Returns:\n","    torch.Tensor: The preprocessed frame as a tensor.\n","    \"\"\"\n","    # Resize the frame to the target size\n","    frame_resized = cv2.resize(frame, (width, height))\n","    # Normalize the frame by dividing by 255\n","    frame_normalized = frame_resized / 255.0\n","    # Get the device from the model parameters\n","    device = next(yolov7_tiny_model.parameters()).device\n","    # Convert the frame to a PyTorch tensor and add a batch dimension\n","    frame_tensor = torch.from_numpy(frame_normalized).float().permute(2, 0, 1).unsqueeze(0)\n","    # Move the tensor to the device and convert to half precision\n","    frame_tensor = frame_tensor.to(device).half()\n","    return frame_tensor\n","\n","import torch\n","from torchvision.ops import nms\n","\n","def process_results(results, confidence_threshold=0.5):\n","    \"\"\"\n","    Process the raw results from the model.\n","\n","    Parameters:\n","    results (torch.Tensor): The raw results from the model.\n","    confidence_threshold (float): The threshold for considering detections.\n","\n","    Returns:\n","    Tuple: A tuple containing confidence scores, bounding boxes, and class predictions.\n","    \"\"\"\n","    # Apply sigmoid to the last dimension (class predictions)\n","    predictions = torch.sigmoid(results[..., 5:])\n","    # Apply sigmoid to the objectness scores\n","    confidence_scores = torch.sigmoid(results[..., 4])\n","    # Apply sigmoid to the bounding box coordinates\n","    boxes = torch.sigmoid(results[..., :4])\n","\n","    # Filter out the results below the confidence threshold\n","    mask = confidence_scores > confidence_threshold\n","    filtered_boxes = boxes[mask]\n","    filtered_scores = confidence_scores[mask]\n","    filtered_predictions = predictions[mask]\n","\n","    # Here we need to determine the class with the maximum prediction for each filtered box\n","    class_predictions = torch.argmax(filtered_predictions, dim=-1)\n","\n","    # Convert to lists\n","    boxes_list = filtered_boxes.tolist()\n","    scores_list = filtered_scores.tolist()\n","    class_predictions_list = class_predictions.tolist()\n","\n","    return scores_list, boxes_list, class_predictions_list\n","\n","\n","\n","def draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name):\n","    \"\"\"\n","    Draw bounding boxes on the frame.\n","\n","    Parameters:\n","    frame (ndarray): The frame to draw the boxes on.\n","    boxes (list): The list of bounding boxes.\n","    original_image_width (int): The original width of the frame.\n","    original_image_height (int): The original height of the frame.\n","    model_name (str): The name of the model to display on the frame.\n","    \"\"\"\n","    # Draw each box on the frame\n","    for box in boxes:\n","        # Scale the box coordinates back to the original frame size\n","        x_center, y_center, width, height = box\n","        x_center *= original_image_width\n","        y_center *= original_image_height\n","        width *= original_image_width\n","        height *= original_image_height\n","        # Convert from center coordinates to top-left coordinates\n","        x = int(x_center - width / 2)\n","        y = int(y_center - height / 2)\n","        # Draw the rectangle on the frame\n","        cv2.rectangle(frame, (x, y), (x + int(width), y + int(height)), (0, 255, 0), 2)\n","    # Put the model name text on the frame\n","    cv2.putText(frame, model_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n","\n","def process_predictions(predictions, class_subset, threshold):\n","    \"\"\"\n","    Process predictions and execute an action if any predictions in a subset exceed a threshold.\n","\n","    Parameters:\n","    predictions (list): List of predictions for all classes.\n","    class_subset (list): List of indices for the subset of classes to check.\n","    prediction_indices (list): List of prediction indices to check within the subset.\n","    threshold (float): The threshold above which a prediction is considered positive.\n","    action (function): A function to execute if a positive prediction is found.\n","    \"\"\"\n","    # Check if any specified prediction index is in the subset and exceeds the threshold\n","    for class_index in predictions:\n","        if class_index in class_subset:\n","            use_large = True\n","            return\n","    use_large = False\n","    return use_large\n","\n","def alert():\n","    use_large = True\n","    print(\"Positive prediction found!\")\n","\n","def evaluate_predictions(predicted_indices, ground_truth_indices):\n","    \"\"\"\n","    Compare given prediction indices for a frame against the ground truth indices.\n","\n","    Parameters:\n","    predicted_indices (list): The list of predicted class indices for the current frame.\n","    ground_truth_indices (list): The list of ground truth class indices for the current frame.\n","\n","    Returns:\n","    Tuple: Counts of true positives, false positives, and false negatives.\n","    \"\"\"\n","    TP = FP = FN = 0\n","    # Convert ground truth and predictions to sets for efficient comparison\n","    predicted_indices_set = set(predicted_indices)\n","    ground_truth_indices_set = set(ground_truth_indices)\n","    '''\n","    print(predicted_indices_set)\n","    print(ground_truth_indices_set)\n","    print(frame_count)\n","    '''\n","    # True Positives (TP): Predicted indices that are in the ground truth\n","    TP = len(predicted_indices_set.intersection(ground_truth_indices_set))\n","\n","    # False Positives (FP): Predicted indices that are not in the ground truth\n","    FP = len(predicted_indices_set.difference(ground_truth_indices_set))\n","\n","    # False Negatives (FN): Ground truth indices that were not predicted\n","    FN = len(ground_truth_indices_set.difference(predicted_indices_set))\n","\n","    return TP, FP, FN\n","\n","def calculate_evaluation_metrics(TP, FP, FN):\n","    \"\"\"\n","    Calculate evaluation metrics based on the counts of true positives, false positives, true negatives, and false negatives.\n","\n","    Parameters:\n","    TP (int): Number of true positives.\n","    FP (int): Number of false positives.\n","    TN (int): Number of true negatives.\n","    FN (int): Number of false negatives.\n","\n","    Returns:\n","    Dict: A dictionary with TPR, Precision, and Recall.\n","    \"\"\"\n","    metrics = {\n","        'TPR': TP / (TP + FN) if (TP + FN) > 0 else 0,\n","        'Precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n","    }\n","    return metrics\n","\n","import torch\n","from torchvision.ops import box_iou, nms\n","\n","def calculate_average_agreement(boxes1, boxes2, predictions1, predictions2, iou_threshold=0.5):\n","    \"\"\"\n","    Calculates the average agreement between two sets of object detection predictions.\n","\n","    Parameters:\n","    - boxes1 (List[List[float]]): Bounding boxes from the first model.\n","    - boxes2 (List[List[float]]): Bounding boxes from the second model.\n","    - predictions1 (List[int]): Class predictions from the first model.\n","    - predictions2 (List[int]): Class predictions from the second model.\n","    - iou_threshold (float): Threshold above which boxes are considered matching.\n","\n","    Returns:\n","    - float: The average agreement score.\n","    \"\"\"\n","    # Ensure that boxes are tensors\n","    if isinstance(boxes1, list):\n","        boxes1 = torch.tensor(boxes1, dtype=torch.float32)\n","    else:\n","        boxes1 = boxes1.clone().detach()\n","\n","    if isinstance(boxes2, list):\n","        boxes2 = torch.tensor(boxes2, dtype=torch.float32)\n","    else:\n","        boxes2 = boxes2.clone().detach()\n","\n","    if boxes1.nelement() == 0 and boxes2.nelement() == 0:\n","        return 1.0  # No agreement if both sets are empty\n","\n","\n","    # Ensure that boxes are not empty and are tensors\n","    # Check if any of the tensors are empty, and if so, return 0.0 as the agreement score\n","    if boxes1.nelement() == 0 or boxes2.nelement() == 0:\n","        return 0.0\n","\n","    boxes1 = torch.tensor(boxes1, dtype=torch.float32)\n","    boxes2 = torch.tensor(boxes2, dtype=torch.float32)\n","\n","    # Calculate the IoU for each pair of boxes\n","    ious = box_iou(boxes1, boxes2)\n","\n","    # Determine which boxes match based on IoU threshold\n","    matches = ious > iou_threshold\n","\n","    total_matches = 0\n","    for i in range(matches.size(0)):\n","        for j in range(matches.size(1)):\n","            if matches[i, j] and predictions1[i] == predictions2[j]:\n","                total_matches += 1\n","\n","    # Calculate the agreement score\n","    total_predictions = max(len(predictions1), len(predictions2))\n","    agreement_score = total_matches / total_predictions if total_predictions > 0 else 0\n","\n","    return agreement_score\n","\n","\n","\n","import csv\n","import time\n","\n","def write_to_csv(data, filename):\n","    with open(filename, mode='w', newline='') as file:\n","        writer = csv.DictWriter(file, fieldnames=['frame_count', 'average_agreement_score'])\n","        writer.writeheader()\n","        for frame_data in data:\n","            writer.writerow(frame_data)\n","\n","def process_video(interval, Skip_Frames=False, Context_Sensitive=False):\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Define the codec\n","    cap = cv2.VideoCapture('file path to .mp4 video') #This is were you change the video input make sure you match the following size\n","    out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (1280, 720))\n","    TP = FP = FN = 0\n","    frame_count = 0\n","    total_time = 0\n","    use_large = False\n","    threshold = 0.5\n","\n","    while cap.isOpened():\n","        start_time = time.time()\n","        ret, frame = cap.read()\n","        frame_count += 1\n","        if not ret:\n","            print(\"Can't receive frame (stream end?). Exiting ...\")\n","            break\n","        if frame_count % 5 != 0 and Skip_Frames:\n","            continue\n","\n","        # Use the larger model every 10 frames\n","        if frame_count % interval == 0:\n","          use_large = True\n","\n","            # Perform inference using the appropriate model\n","        if use_large:\n","            # Preprocess the current frame to prepare for model input\n","            frame_tensor = preprocess_frame(frame, 640, 640)\n","            results = yolov7_model(frame_tensor)\n","            results = results[0]\n","            model_name = \"yolov7\"\n","        else:\n","            # Preprocess the current frame to prepare for model input\n","            frame_tensor = preprocess_frame(frame, 608, 608)\n","            results = yolov7_tiny_model(frame_tensor)\n","            results = results[0]\n","            model_name = \"yolov7-tiny\"\n","\n","        # Check if results are in a list and select the first element\n","        if isinstance(results, list):\n","            results = results[0]\n","\n","        # Process the results to get confidence scores, bounding boxes, and predictions\n","        confidence_scores, boxes, predictions = process_results(results, threshold)\n","\n","        # Draw bounding boxes on the frame\n","        draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","        # Assuming class_labels is defined elsewhere and contains class indices as keys\n","        class_indices = list(class_labels.keys())  # Get all class indices\n","\n","        # Exclude the first 10 classes for processing\n","        subset_indices = class_indices[10:]\n","\n","        if Context_Sensitive:\n","          # Process predictions to check for any detections in the subset of classes\n","          process_predictions(predictions, subset_indices, threshold)\n","        else:\n","          use_large = False\n","\n","        # Write the processed frame to the output video\n","        out.write(frame)\n","\n","\n","        frame_predictions_indices = frame_predictions[str(frame_count)]\n","\n","        # Draw bounding boxes on the frame\n","        draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","        # Evaluate the predictions against the ground truth for the current frame\n","        frame_TP, frame_FP, frame_FN = evaluate_predictions(predictions, frame_predictions_indices)\n","\n","        # Update the overall confusion matrix counters\n","        TP += frame_TP\n","        FP += frame_FP\n","        FN += frame_FN\n","\n","        # Write the processed frame to the output video\n","        out.write(frame)\n","\n","\n","        # Calculate processing time and update total time\n","        end_time = time.time()\n","        total_time += (end_time - start_time)\n","\n","\n","    # Release video resources\n","    cap.release()\n","    out.release()\n","    print(frame_count)\n","    print(total_time)\n","    # Calculate FPS and evaluation metrics\n","    fps = frame_count / total_time\n","    evaluation_metrics = calculate_evaluation_metrics(TP, FP, FN)\n","    return evaluation_metrics, fps\n","\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4109001,"status":"ok","timestamp":1699760409358,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"YufYPoMrzzF9","outputId":"fc94a4ac-7df1-45e2-fdea-3d2c4e35ca69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Can't receive frame (stream end?). Exiting ...\n","5596\n","528.2009534835815\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","510.1961884498596\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","504.02336716651917\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","113.66189193725586\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","107.37122964859009\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","103.84329557418823\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","612.0269241333008\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","612.6276412010193\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","611.8702168464661\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","124.1579692363739\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","124.21319603919983\n","Can't receive frame (stream end?). Exiting ...\n","5596\n","124.02974963188171\n","Processing complete. Metrics written to evaluation_metrics.csv\n"]}],"source":["#Run the Model Evaluator on \"large_model_intervals\" and all combinations of \"schemes\"\n","#Outputs a CSV file with the results\n","\n","import cv2\n","import torch\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import json\n","\n","large_model_intervals = [10, 20, 40]\n","schemes = [[False, False], [True, False], [False, True], [True, True]]\n","# Load pre-trained models\n","yolov7_model = torch.load('yolov7-e6e .pt file path')['model']\n","yolov7_tiny_model = torch.load('yolov7-tiny .pt file path')['model']\n","# Original dimensions of the video frames\n","original_image_height = 720\n","original_image_width = 1280\n","\n","# Load the ground truth predictions from a JSON file\n","with open('file path to .json updated e6e predicitons', 'r') as f:\n","    frame_predictions = json.load(f)\n","\n","# Set the models to evaluation mode\n","yolov7_model.eval()\n","yolov7_tiny_model.eval()\n","\n","# Set the threshold for detection\n","threshold = 0.5  # Change this value to adjust sensitivity\n","# Process video for each interval and write results to CSV\n","with open('evaluation_metrics.csv', 'w', newline='') as csvfile:\n","    fieldnames = ['Context_Sense', 'Skip_Frame', 'Interval', 'TPR', 'Precision', 'FPS']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","    for scheme in schemes:\n","        Skip_Frames = scheme[0]\n","        Context_Sense = scheme[1]\n","        for interval in large_model_intervals:\n","            metrics, fps = process_video(interval, Skip_Frames, Context_Sense)\n","            writer.writerow({'Context_Sense': f\"{Context_Sense}\", 'Skip_Frame': f\"{Skip_Frames}\", 'Interval': f\"{interval}\", 'TPR': metrics['TPR'], 'Precision': metrics['Precision'], 'FPS': fps})\n","\n","print(\"Processing complete. Metrics written to evaluation_metrics.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505509,"status":"ok","timestamp":1699835436956,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"1at1nKAFWdRz","outputId":"0b0ae3fe-6c4d-4505-bc09-e8e6bf8ceb09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Can't receive frame (stream end?). Exiting ...\n","5596\n","503.30618810653687\n","TPR: 0.7985921655428012\n","Precision: 0.4894625589576136\n","FPS: 11.118480424515408\n"]}],"source":["#To do a single run of the model evaluator\n","import cv2\n","import torch\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import json\n","\n","large_model_intervals = [10, 20, 40]\n","schemes = [[False, False], [True, False], [False, True], [True, True]]\n","# Load pre-trained models\n","yolov7_model = torch.load('yolov7-e6e .pt file path')['model']\n","yolov7_tiny_model = torch.load('yolov7-tiny .pt file path')['model']\n","# Original dimensions of the video frames\n","original_image_height = 720\n","original_image_width = 1280\n","\n","# Load the ground truth predictions from a JSON file\n","with open('file path to .json updated e6e predicitons', 'r') as f:\n","    frame_predictions = json.load(f)\n","\n","# Set the models to evaluation mode\n","yolov7_model.eval()\n","yolov7_tiny_model.eval()\n","\n","# Set the threshold for detection\n","threshold = 0.5  # Change this value to adjust sensitiviy\n","\n","metrics, fps = process_video(10, False, False)\n","\n","print(f\"TPR: {metrics['TPR']}\")\n","print(f\"Precision: {metrics['Precision']}\")\n","print(f\"FPS: {fps}\")\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":575986,"status":"ok","timestamp":1699735846339,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"SgvvNpYrv-YL","outputId":"9018b7f9-a249-4466-f3aa-f723e7c7f3b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Failed to grab a frame\n","True Positive Rate (TPR): 1.0\n","Precision: 1.0\n","Final Frame Count: 5596\n"]}],"source":["#Run for a single instance without outside function\n","import cv2\n","import torch\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import json\n","\n","# Load pre-trained models\n","yolov7_model = torch.load('yolov7-e6e .pt file path')['model']\n","yolov7_tiny_model = torch.load('yolov7-tiny .pt file path')['model']\n","\n","\n","# Set the models to evaluation mode\n","yolov7_model.eval()\n","# Set the threshold for detection\n","threshold = 0.5  # Change this value to adjust sensitivity\n","\n","# Initialize frame count and flags\n","frame_count = 0\n","use_large = False\n","\n","\n","# Original dimensions of the video frames\n","original_image_height = 720\n","original_image_width = 1280\n","\n","# Initialize confusion matrix counters\n","TP = FP = FN = 0\n","\n","\n","# Load the ground truth predictions from a JSON file\n","with open('file path to .json updated e6e predicitons', 'r') as f:\n","    frame_predictions = json.load(f)\n","\n","# Open the video file\n","cap = cv2.VideoCapture('file path to .mp4 video')\n","\n","# Set up the VideoWriter object to save the output video\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Define the codec\n","out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (1280, 720))  # Set the output file name, codec, fps, and resolution\n","\n","# Process video frames\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    frame_count += 1\n","    if not ret:\n","        print(\"Failed to grab a frame\")\n","        break  # Exit the loop if no frame is grabbed\n","    if frame_count % 5 != 0:\n","        continue\n","\n","\n","\n","    # Use the larger model every 10 frames\n","    if frame_count % 40 == 0:\n","        use_large = True\n","\n","\n","\n","    # Perform inference using the appropriate model\n","    if use_large:\n","        # Preprocess the current frame to prepare for model input\n","        frame_tensor = preprocess_frame(frame, 640, 640)\n","        results = yolov7_model(frame_tensor)\n","        results = results[0]\n","        model_name = \"yolov7\"\n","    else:\n","        # Preprocess the current frame to prepare for model input\n","        frame_tensor = preprocess_frame(frame, 608, 608)\n","        results = yolov7_tiny_model(frame_tensor)\n","        model_name = \"yolov7-tiny\"\n","\n","    # Check if results are in a list and select the first element\n","    if isinstance(results, list):\n","        results = results[0]\n","\n","    # Process the results to get confidence scores, bounding boxes, and predictions\n","    confidence_scores, boxes, predictions = process_results(results, threshold)\n","\n","    # Draw bounding boxes on the frame\n","    draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","    # Assuming class_labels is defined elsewhere and contains class indices as keys\n","    class_indices = list(class_labels.keys())  # Get all class indices\n","\n","    # Exclude the first 10 classes for processing\n","    subset_indices = class_indices[10:]\n","\n","    # Process predictions to check for any detections in the subset of classes\n","    process_predictions(predictions, subset_indices, threshold)\n","\n","\n","    frame_predictions_indices = frame_predictions[str(frame_count)]\n","\n","    # Draw bounding boxes on the frame\n","    draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","    # Evaluate the predictions against the ground truth for the current frame\n","    frame_TP, frame_FP, frame_FN = evaluate_predictions(predictions, frame_predictions_indices)\n","\n","    # Update the overall confusion matrix counters\n","    TP += frame_TP\n","    FP += frame_FP\n","    FN += frame_FN\n","\n","    # Write the processed frame to the output video\n","    out.write(frame)\n","\n","# Release video resources\n","cap.release()\n","out.release()\n","\n","\n","# Calculate and print evaluation metrics\n","evaluation_metrics = calculate_evaluation_metrics(TP, FP, FN)\n","print(f\"True Positive Rate (TPR): {evaluation_metrics['TPR']}\")\n","print(f\"Precision: {evaluation_metrics['Precision']}\")\n","print(\"Final Frame Count:\", frame_count)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571468,"status":"ok","timestamp":1699732000407,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"6hcjSLaYheSO","outputId":"d7669bf7-6ba2-4268-a8d1-61c710149d94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Failed to grab a frame\n","True Positive Rate (TPR): 0.7785016286644951\n","Precision: 0.027680547691357388\n","Final Frame Count: 5596\n"]}],"source":["#producing prediction dictionary\n","import cv2\n","import torch\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import json\n","\n","# Load pre-trained models\n","yolov7_model = torch.load('yolov7-e6e .pt file path')['model']\n","yolov7_tiny_model = torch.load('yolov7-tiny .pt file path')['model']\n","\n","frame_predictions_dict = {}\n","\n","# Load the class labels\n","# Set the models to evaluation mode\n","yolov7_model.eval()\n","# Set the threshold for detection\n","threshold = 0.5  # Change this value to adjust sensitivity\n","\n","# Initialize frame count and flags\n","frame_count = 0\n","use_large = False\n","\n","\n","# Original dimensions of the video frames\n","original_image_height = 720\n","original_image_width = 1280\n","\n","# Initialize confusion matrix counters\n","TP = FP = FN = 0\n","\n","\n","# Load the ground truth predictions from a JSON file\n","with open('file path to .json updated e6e predicitons', 'r') as f:\n","    frame_predictions = json.load(f)\n","\n","# Open the video file\n","cap = cv2.VideoCapture('file path to .mp4 video')\n","\n","# Set up the VideoWriter object to save the output video\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Define the codec\n","out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (1280, 720))  # Set the output file name, codec, fps, and resolution\n","\n","# Process video frames\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    frame_count += 1\n","    if not ret:\n","        print(\"Failed to grab a frame\")\n","        break  # Exit the loop if no frame is grabbed\n","    if frame_count % 5 != 0 and False:\n","        continue\n","\n","\n","\n","    # Use the larger model every 10 frames\n","    if frame_count % 40 == 0:\n","        use_large = True\n","\n","\n","\n","    # Perform inference using the appropriate model\n","    if use_large or True:\n","        # Preprocess the current frame to prepare for model input\n","        frame_tensor = preprocess_frame(frame, 640, 640)\n","        results = yolov7_model(frame_tensor)\n","        results = results[0]\n","        model_name = \"yolov7\"\n","    else:\n","        # Preprocess the current frame to prepare for model input\n","        frame_tensor = preprocess_frame(frame, 608, 608)\n","        results = yolov7_tiny_model(frame_tensor)\n","        model_name = \"yolov7-tiny\"\n","\n","    # Check if results are in a list and select the first element\n","    if isinstance(results, list):\n","        results = results[0]\n","\n","    # Process the results to get confidence scores, bounding boxes, and predictions\n","    confidence_scores, boxes, predictions = process_results(results, threshold)\n","\n","    # Draw bounding boxes on the frame\n","    draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","    # Assuming class_labels is defined elsewhere and contains class indices as keys\n","    class_indices = list(class_labels.keys())  # Get all class indices\n","\n","    # Exclude the first 10 classes for processing\n","    subset_indices = class_indices[10:]\n","\n","    # Process predictions to check for any detections in the subset of classes\n","    process_predictions(predictions, subset_indices, threshold)\n","\n","\n","    frame_predictions_indices = frame_predictions[str(frame_count)]\n","\n","    # Draw bounding boxes on the frame\n","    draw_boxes_on_frame(frame, boxes, original_image_width, original_image_height, model_name)\n","\n","    frame_predictions_dict[str(frame_count)] = predictions\n","\n","    # Evaluate the predictions against the ground truth for the current frame\n","    frame_TP, frame_FP, frame_FN = evaluate_predictions(predictions, frame_predictions_indices)\n","\n","    # Update the overall confusion matrix counters\n","    TP += frame_TP\n","    FP += frame_FP\n","    FN += frame_FN\n","\n","    # Write the processed frame to the output video\n","    out.write(frame)\n","\n","# Release video resources\n","cap.release()\n","out.release()\n","\n","with open('/content/drive/MyDrive/EdgeAI/Collab Folder/BigModelOutput/frame_predictions_update_e6e.json', 'w') as outfile:\n","    json.dump(frame_predictions_dict, outfile)\n","\n","# Calculate and print evaluation metrics\n","evaluation_metrics = calculate_evaluation_metrics(TP, FP, FN)\n","print(f\"True Positive Rate (TPR): {evaluation_metrics['TPR']}\")\n","print(f\"Precision: {evaluation_metrics['Precision']}\")\n","print(\"Final Frame Count:\", frame_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45173,"status":"ok","timestamp":1699571788163,"user":{"displayName":"XB HG","userId":"16419867972515022604"},"user_tz":360},"id":"T2FZtdqfQ0Rb","outputId":"fcd791e4-c6ce-41c9-e2e8-d1f59a72c008"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-26-3ad473cb1f45>:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  boxes1 = torch.tensor(boxes1, dtype=torch.float32)\n","<ipython-input-26-3ad473cb1f45>:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  boxes2 = torch.tensor(boxes2, dtype=torch.float32)\n"]},{"name":"stdout","output_type":"stream","text":["Data written to CSV file\n"]}],"source":["#Capturing model agreement on frame bases\n","import cv2\n","import torch\n","import sys\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import json\n","\n","# Load pre-trained models\n","yolov7_model = torch.load('yolov7-e6e .pt file path')['model']\n","yolov7_tiny_model = torch.load('yolov7-tiny .pt file path')['model']\n","\n","# Set the threshold for detection\n","threshold = 0.5  # Change this value to adjust sensitivity\n","\n","# Initialize frame count and flags\n","frame_count = 0\n","\n","# Initialize a list to hold the data dictionaries\n","data_to_write = []\n","\n","# Original dimensions of the video frames\n","original_image_height = 720\n","original_image_width = 1280\n","\n","# Initialize confusion matrix counters\n","TP = FP = TN = FN = 0\n","\n","# Load the ground truth predictions from a JSON file\n","with open('file path to .json updated e6e predicitons', 'r') as f:\n","    frame_predictions = json.load(f)\n","\n","# Open the video file\n","cap = cv2.VideoCapture('file path to .mp4 video')\n","\n","# Set up the VideoWriter object to save the output video\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Define the codec\n","out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (1280, 720))  # Set the output file name, codec, fps, and resolution\n","\n","# Process video frames\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    frame_count += 1\n","    if not ret:\n","        print(\"Failed to grab a frame\")\n","        break  # Exit the loop if no frame is grabbed\n","\n","\n","\n","    # Preprocess the current frame to prepare for model input\n","    frame_tensor = preprocess_frame(frame, 608, 608)\n","\n","    results_large = yolov7_model(frame_tensor)\n","    results_tiny = yolov7_tiny_model(frame_tensor)\n","\n","    # Check if results are in a list and select the first element\n","    if isinstance(results_large, list):\n","        results_large = results_large[0]\n","    if isinstance(results_tiny, list):\n","        results_tiny = results_tiny[0]\n","    confidence_boxes, boxes_tiny, predictions_tiny = process_results(results_tiny, 0.5)\n","    confidence_boxes, boxes_large, predictions_large = process_results(results_large, 0.5)\n","\n","    average_agreement = calculate_average_agreement(boxes_tiny, boxes_large, predictions_tiny, predictions_large)\n","\n","    data_to_write.append({\n","        'frame_count': frame_count,\n","        'average_agreement_score': average_agreement\n","    })\n","\n","    # Evaluate the predictions against the ground truth for the current frame\n","    frame_TP, frame_FP, frame_TN, frame_FN = evaluate_predictions(frame_count, predictions_large, frame_predictions, threshold)\n","\n","    if frame_count == 1000:\n","        write_to_csv(data_to_write, \"Model_Agreement_Object_Detection.csv\")\n","        print(\"Data written to CSV file\")\n","        break\n","\n","    # Draw bounding boxes on the frame\n","\n","\n","\n","    # Draw bounding boxes and predictions on the frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dquVBvyyYY8T"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1D4TdTbhWW12n9GtjObAFu8yA_OsKXv7f","timestamp":1699288850763},{"file_id":"1CB0Wi58_O8rfTAQObhXmMytwoGz9_TwO","timestamp":1699226915278}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
